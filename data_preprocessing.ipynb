{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a665c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Preprocessing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Meet\\AppData\\Local\\Temp\\ipykernel_17560\\3743948683.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace(team_mapping, inplace=True)\n",
      "C:\\Users\\Meet\\AppData\\Local\\Temp\\ipykernel_17560\\3743948683.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  matches['venue'].fillna('Unknown Venue', inplace=True)\n",
      "C:\\Users\\Meet\\AppData\\Local\\Temp\\ipykernel_17560\\3743948683.py:62: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['winner'].fillna('No Result', inplace=True)\n",
      "C:\\Users\\Meet\\AppData\\Local\\Temp\\ipykernel_17560\\3743948683.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_df.rename(columns={\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed DataFrame ready for ML (Shape: (123428, 10))\n",
      "--- Starting Model Training ---\n",
      "Starting XGBoost Model Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Meet\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:48:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete.\n",
      "Model Accuracy on Test Set: 85.19%\n",
      "\n",
      "All required files: xgb_ipl_predictor.joblib, column_transformer.joblib, and analysis_data.joblib have been saved.\n",
      "--- Training and Saving Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib      #\n",
    "import xgboost as xgb # Required library\n",
    "from sklearn.model_selection import train_test_split        #        \n",
    "from sklearn.preprocessing import OneHotEncoder             #\n",
    "from sklearn.compose import ColumnTransformer               #\n",
    "from sklearn.metrics import accuracy_score              #\n",
    "import os\n",
    "\n",
    "def preprocess_data(matches_path, deliveries_path):\n",
    "    \"\"\"\n",
    "    Loads, cleans, and preprocesses IPL data for second-inning Win Probability prediction.\n",
    "    \"\"\"\n",
    "    # 1. Load datasets\n",
    "    try:\n",
    "        matches = pd.read_csv(\"./data/matches.csv\")\n",
    "        deliveries = pd.read_csv(\"./data/deliveries.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: One or both files not found at the expected path: {matches_path}, {deliveries_path}\")\n",
    "        # Return empty dataframes to prevent downstream errors\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # 2. Standardize Column Names and Team Names\n",
    "    deliveries.rename(columns={'match_id': 'id'}, inplace=True)\n",
    "    deliveries.columns = deliveries.columns.str.lower()\n",
    "    matches.columns = matches.columns.str.lower()\n",
    "    \n",
    "    team_mapping = {\n",
    "        'Delhi Daredevils': 'Delhi Capitals',\n",
    "        'Deccan Chargers': 'Sunrisers Hyderabad',\n",
    "        'Kings XI Punjab': 'Punjab Kings',\n",
    "        'Pune Warriors': 'Rising Pune Supergiants',\n",
    "        'Rising Pune Supergiant': 'Rising Pune Supergiants',\n",
    "        'Gujarat Lions': 'Gujarat Titans',\n",
    "        'Royal Challengers Bangalore': 'Royal Challengers Bengaluru'\n",
    "    }\n",
    "    \n",
    "    for df in [deliveries, matches]:\n",
    "        for col in ['batting_team', 'bowling_team', 'winner', 'team1', 'team2']:\n",
    "            if col in df.columns:\n",
    "                df[col].replace(team_mapping, inplace=True)\n",
    "                \n",
    "    # CRITICAL: Impute missing venue before merge\n",
    "    matches['venue'].fillna('Unknown Venue', inplace=True)\n",
    "    \n",
    "    # 3. Feature Engineering\n",
    "    \n",
    "    # Calculate Target Score (First Inning Total)\n",
    "    first_inning_scores = deliveries[deliveries['inning'] == 1].groupby('id')['total_runs'].sum().reset_index()\n",
    "    first_inning_scores.columns = ['id', 'target']\n",
    "    \n",
    "    # Merge all data\n",
    "    df = deliveries.merge(matches, on='id')\n",
    "    df = df.merge(first_inning_scores, on='id')\n",
    "\n",
    "    # Filter for Second Innings and remove Super Overs\n",
    "    df = df[df['inning'] == 2]\n",
    "    df = df[df['super_over'] == 'N'].copy()\n",
    "    \n",
    "    # FIX: Handle missing 'winner' values (matches with no result)\n",
    "    df['winner'].fillna('No Result', inplace=True) \n",
    "    df = df[df['winner'] != 'No Result'].copy()\n",
    "\n",
    "    # Calculate Core Features\n",
    "    df['current_score'] = df.groupby('id')['total_runs'].cumsum()\n",
    "    df['wickets_taken'] = df.groupby('id')['is_wicket'].cumsum() \n",
    "    df['wickets_left'] = 10 - df['wickets_taken']\n",
    "    df['balls_left'] = 120 - (df['over'] * 6 + df['ball'])\n",
    "    \n",
    "    # Calculate Run Rate Features\n",
    "    balls_bowled = 120 - df['balls_left']\n",
    "    df['current_run_rate'] = np.where(balls_bowled > 0, (df['current_score'] * 6) / balls_bowled, 0)\n",
    "    df['required_run_rate'] = np.where(df['balls_left'] > 0, (df['target'] - df['current_score']) / (df['balls_left'] / 6), np.inf)\n",
    "\n",
    "    # Create Target Variable (1 if chasing team wins, 0 otherwise)\n",
    "    df['result'] = np.where(df['winner'] == df['batting_team'], 1, 0)\n",
    "    \n",
    "    # 4. Final Cleanup\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Drop rows where essential features are still NaN, and remove balls that completed the match (0 balls left)\n",
    "    df.dropna(subset=['venue', 'batting_team', 'bowling_team', 'current_score', 'wickets_left', 'balls_left', 'target', 'current_run_rate', 'required_run_rate', 'result'], inplace=True)\n",
    "    df = df[df['balls_left'] > 0]\n",
    "    \n",
    "    # 5. Select features and rename\n",
    "    processed_df = df[[\n",
    "        'venue', 'batting_team', 'bowling_team', 'current_score',\n",
    "        'wickets_left', 'balls_left', 'target', 'current_run_rate',\n",
    "        'required_run_rate', 'result'\n",
    "    ]]\n",
    "    \n",
    "    processed_df.rename(columns={\n",
    "        'target': 'runs_to_chase',\n",
    "        'wickets_left': 'wickets_remaining'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    analysis_df = df[[\n",
    "        'id', 'over', 'ball', 'batting_team', 'bowling_team', \n",
    "        'current_score', 'wickets_taken', 'target', 'total_runs', 'result', 'venue'\n",
    "    ]].copy()\n",
    "    \n",
    "    analysis_df.rename(columns={'id': 'ID', 'over': 'overs', 'ball': 'ballnumber', 'total_runs': 'total_run'}, inplace=True)\n",
    "    \n",
    "    return processed_df, analysis_df\n",
    "\n",
    "\n",
    "def train_and_save_model(processed_df, analysis_df):\n",
    "    \"\"\"Trains the XGBoost model and saves all necessary files.\"\"\"\n",
    "    \n",
    "    if processed_df.empty:\n",
    "        print(\"Training skipped: Processed DataFrame is empty.\")\n",
    "        return\n",
    "        \n",
    "    # 1. Define Features (X) and Target (Y)\n",
    "    X = processed_df.drop(columns=['result'])\n",
    "    Y = processed_df['result']\n",
    "    \n",
    "    # 2. Setup the Column Transformer (One-Hot Encoder)\n",
    "    categorical_features = ['venue', 'batting_team', 'bowling_team']\n",
    "    trf = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ohe', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # 3. Apply the transformation and save the fitted transformer\n",
    "    X_transformed = trf.fit_transform(X)\n",
    "    joblib.dump(trf, 'column_transformer.joblib')\n",
    "    \n",
    "    # 4. Split the data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_transformed, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 5. Initialize and Train XGBoost Classifier\n",
    "    # NOTE: Set objective to binary:logistic for classification\n",
    "    xgb_classifier = xgb.XGBClassifier(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.05, \n",
    "        max_depth=5, \n",
    "        random_state=42,\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    print(\"Starting XGBoost Model Training...\")\n",
    "    xgb_classifier.fit(X_train, Y_train)\n",
    "    print(\"Training Complete.\")\n",
    "    \n",
    "    # 6. Evaluate and Save the model\n",
    "    Y_pred = xgb_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    print(f\"Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Save the model with the correct XGBoost name\n",
    "    joblib.dump(xgb_classifier, 'xgb_ipl_predictor.joblib')\n",
    "    joblib.dump(analysis_df, 'analysis_data.joblib') \n",
    "    \n",
    "    print(\"\\nAll required files: xgb_ipl_predictor.joblib, column_transformer.joblib, and analysis_data.joblib have been saved.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ASSUME files are in the current working directory\n",
    "    matches_csv = 'matches.csv'\n",
    "    deliveries_csv = 'deliveries.csv'\n",
    "    \n",
    "    print(\"--- Starting Data Preprocessing ---\")\n",
    "    processed_df, analysis_df = preprocess_data(matches_csv, deliveries_csv)\n",
    "    print(f\"Preprocessed DataFrame ready for ML (Shape: {processed_df.shape})\")\n",
    "    \n",
    "    if not processed_df.empty:\n",
    "        print(\"--- Starting Model Training ---\")\n",
    "        train_and_save_model(processed_df, analysis_df)\n",
    "    else:\n",
    "        print(\"Training cannot proceed due to empty DataFrame.\")\n",
    "    print(\"--- Training and Saving Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabf7c2",
   "metadata": {},
   "source": [
    "final data processing and testing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
